{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Convolution2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nConvolution2D( no_of_feture_detectors, \\n              no_of_rows_in_feature_detectors, \\n              no_of_cols_in_feature_detectors, \\n              border_mode = 'same',\\n              activation = 'relu'              #As we will use max pooling.\\n             input_shape = formate of input images./ ( 3, 256, 256 ) ) # Here 3 is for coloured Images and it should be 1 for\\n                                                                       # black and white images.\\n                                                                # 256 and 256 are the dimensions of the array in the channel. \\n                                                                \\n            but there is diffrence. We will be using tensor flow and in tensor flow. the way of writing the shapes in\\n            input_shape is opposite. ( 256, 256, 3 )\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "nb_filter = no. of feture detectors( for each feature map we will be having diffrent feture detectors.)\n",
    "So, no. of desired feture map = no. of feture detectors.\n",
    "\n",
    "#Note :- filter/Convolution kernal both are another names of feature detectors.\n",
    "\n",
    "nb_row = no. of rows in feture detectors.\n",
    "nb_col = no. of columns in feture detectors.\n",
    "border_modes = 'same' describes how the feture detectors are going to handle the borders of input images. \n",
    "                Same is the default value.\n",
    "\n",
    "input_shape = input_shape is to set the limit for all input images. as all images has differnt size and formats to generalize\n",
    "                all images formate into one formate we use input_shape\n",
    "                \n",
    "If Image = 'Coloured_Image':\n",
    "    input image will be converted into 3-d array( R, G and B )\n",
    "\n",
    "elif Image == 'Black&White_Image':\n",
    "    input image will be converted into 2-d array.\n",
    "'''\n",
    "\n",
    "'''\n",
    "Convolution2D( no_of_feture_detectors, \n",
    "              no_of_rows_in_feature_detectors, \n",
    "              no_of_cols_in_feature_detectors, \n",
    "              border_mode = 'same',\n",
    "              activation = 'relu'              #As we will use max pooling.\n",
    "             input_shape = formate of input images./ ( 3, 256, 256 ) ) # Here 3 is for coloured Images and it should be 1 for\n",
    "                                                                       # black and white images.\n",
    "                                                                # 256 and 256 are the dimensions of the array in the channel. \n",
    "                                                                \n",
    "            but there is diffrence. We will be using tensor flow and in tensor flow. the way of writing the shapes in\n",
    "            input_shape is opposite. ( 256, 256, 3 )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(64, 64, 3..., activation=\"relu\", padding=\"same\")`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "cnn.add( Convolution2D( 32, 3, 3, border_mode = 'same', input_shape = ( 64, 64, 3  ), activation = 'relu' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Pooling the CNN\n",
    "cnn.add( MaxPooling2D( pool_size = ( 2, 2 ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flattening - Input Layer for ANN.\n",
    "\n",
    "cnn.add( Flatten() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"relu\", units=128)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nHere, output_dim in Dense() is hit and trail value. We need to put the value and test. If the output_dim( Which is ulimately\\n        going to be the input of hidden layer) is near and >= 100 and output_dim must be a value which is the power of 2\\n        then, It is Idle.\\n        \\n#we don't know the input_layer because, our input node of 1st layer is nothing but the Flattening result( which is m*n of relu\\n        shape ).\\n\\n#we don't need to write init as we use it set out 1st nodes value somewhere around 0 but here we already have dynamic node \\n    values.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Full Connection/ Applyin ANN.\n",
    "\n",
    "cnn.add( Dense( output_dim = 128, activation = 'relu') )\n",
    "\n",
    "'''\n",
    "Here, output_dim in Dense() is hit and trail value. We need to put the value and test. If the output_dim( Which is ulimately\n",
    "        going to be the input of hidden layer) is near and >= 100 and output_dim must be a value which is the power of 2\n",
    "        then, It is Idle.\n",
    "        \n",
    "#we don't know the input_layer because, our input node of 1st layer is nothing but the Flattening result( which is m*n of relu\n",
    "        shape ).\n",
    "\n",
    "#we don't need to write init as we use it set out 1st nodes value somewhere around 0 but here we already have dynamic node \n",
    "    values.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", units=1)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n#We are considering sigmoid as as output either we have Cat or Dog. Only 2 Categories. So, In Sigmoid.\\n\\nif( Input_Image == 'Dog' ):\\n    Output of sigmoid can be either True or False.\\n    \\n    if( Sigmoid_value == False ):\\n        Then, It is not a Dog which means it's a cat.\\n    \\n    else:\\n        It is a Dog.\\n\\nelse:\\n    if( Sigmoid_value == False ):\\n        Then, It is not a Cat which means it's a Dog.\\n    \\n    else:\\n        It is a Cat.\\n\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.add( Dense( output_dim = 1, activation = 'sigmoid' ) )\n",
    "\n",
    "'''\n",
    "#We are considering sigmoid as as output either we have Cat or Dog. Only 2 Categories. So, In Sigmoid.\n",
    "\n",
    "if( Input_Image == 'Dog' ):\n",
    "    Output of sigmoid can be either True or False.\n",
    "    \n",
    "    if( Sigmoid_value == False ):\n",
    "        Then, It is not a Dog which means it's a cat.\n",
    "    \n",
    "    else:\n",
    "        It is a Dog.\n",
    "\n",
    "else:\n",
    "    if( Sigmoid_value == False ):\n",
    "        Then, It is not a Cat which means it's a Dog.\n",
    "    \n",
    "    else:\n",
    "        It is a Cat.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling the model.\n",
    "\n",
    "cnn.compile( optimizer = 'adam', loss = 'binary_crossentropy', metrics = [ 'accuracy' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWe need to do Data Pre-Processing as we know. CNN is going to find the patterns in the pixel and based on those patterns.\\nIt is going to process our test data and give the result.\\n\\nWe are giving 12500 data which is not sufficient for CNN to understand the pattern properly. For maximum CNN\\nunderstnading of patterns. We need more images.\\n\\nTo arrange images is not a easy task.\\nSo, We need to Image Augmentation. Image Augmentation will rotate( Left, Right, Down ), flip( Mirror Image ), \\nblur, etc on our images and creates multiple images from 1 image and the best thing is It happends dynamically.\\nIt doen't store those augmented images and thus, it avoids overfitting also.\\n\\nand thus, our problem of large Image requirement gets solve. That's why we need Image Augmentation Technique.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Image Pre-Processing/ Augmentation.\n",
    "\n",
    "'''\n",
    "We need to do Data Pre-Processing as we know. CNN is going to find the patterns in the pixel and based on those patterns.\n",
    "It is going to process our test data and give the result.\n",
    "\n",
    "We are giving 12500 data which is not sufficient for CNN to understand the pattern properly. For maximum CNN\n",
    "understnading of patterns. We need more images.\n",
    "\n",
    "To arrange images is not a easy task.\n",
    "So, We need to Image Augmentation. Image Augmentation will rotate( Left, Right, Down ), flip( Mirror Image ), \n",
    "blur, etc on our images and creates multiple images from 1 image and the best thing is It happends dynamically.\n",
    "It doen't store those augmented images and thus, it avoids overfitting also.\n",
    "\n",
    "and thus, our problem of large Image requirement gets solve. That's why we need Image Augmentation Technique.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,                 #Divide the Image by 255 to make all the pixel value from 0 to 1 as RGBrange \n",
    "                                                                                                    #is from 0 to 255. \n",
    "        shear_range=0.2,                #Shering Intensiry\n",
    "        zoom_range=0.2,                 #Zooming Intensity\n",
    "        horizontal_flip=True,           #Flipping the Image Horizintaly.\n",
    "        vertical_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_set = train_datagen.flow_from_directory('DataSet/train_data',               #path for the training set\n",
    "                                                target_size=(64, 64),           #Expected Size/dimension of Image.\n",
    "                                                                                       #same as input_shape of Convolution2D()\n",
    "                                                batch_size=32,                    #Every time 32 Images will be taken in batch.\n",
    "                                                class_mode='binary')            #As output we have only 2 types cats and dogs.\n",
    "                                                                                        #so, class_mode = 'binary'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 430 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_set = test_datagen.flow_from_directory('DataSet/test_data',\n",
    "                                            target_size=(64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  \"\"\"\n",
      "c:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras.pre..., validation_data=<keras.pre..., steps_per_epoch=781, epochs=1, validation_steps=430)`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "781/781 [==============================] - 170s 218ms/step - loss: 0.6344 - accuracy: 0.6445 - val_loss: 0.5884 - val_accuracy: 0.6859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x158fd4a0f48>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit_generator(  train_set ,\n",
    "                    samples_per_epoch = 25000,               #No of Images in training data folder\n",
    "                    nb_epoch = 1,                           #no. of epoch/ no/ of iteration of CNN.\n",
    "                    validation_data = test_set,\n",
    "                    nb_val_samples = 430)               #No of Images in testing data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x1588704B8C8>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_test_image = image.load_img( 'DataSet/Single_prediction/Cat_image.jpg'\n",
    "                              , target_size = ( 64, 64 ) )    \n",
    "\n",
    "#we trained our image with target_size as (64, 64). So, while testing also target must be same as training target_size.\n",
    "\n",
    "cat_test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x15886EB1908>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_test_image = image.load_img( 'DataSet/Single_prediction/Dog_image.jpg'\n",
    "                              , target_size = ( 64, 64 ) )\n",
    "dog_test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to convert out image into RGB pixels. as while creating CNN Layer we gave input_size as ( 64, 64, 3 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test_image_arr = image.img_to_array( cat_test_image )\n",
    "dog_test_image_arr = image.img_to_array( dog_test_image )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_test_image_arr.shape = (64, 64, 3) and type( cat_test_image_arr ) = <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[179., 184., 154.],\n",
       "        [185., 190., 160.],\n",
       "        [168., 173., 143.],\n",
       "        ...,\n",
       "        [ 97.,  80.,  26.],\n",
       "        [111.,  95.,  44.],\n",
       "        [119., 106.,  54.]],\n",
       "\n",
       "       [[185., 190., 160.],\n",
       "        [194., 199., 169.],\n",
       "        [172., 177., 147.],\n",
       "        ...,\n",
       "        [160., 139.,  72.],\n",
       "        [154., 136.,  72.],\n",
       "        [140., 126.,  61.]],\n",
       "\n",
       "       [[189., 194., 164.],\n",
       "        [198., 203., 173.],\n",
       "        [174., 179., 149.],\n",
       "        ...,\n",
       "        [167., 137.,  83.],\n",
       "        [158., 137.,  74.],\n",
       "        [147., 132.,  67.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  1.,   3.,   2.],\n",
       "        [ 22.,  20.,   5.],\n",
       "        [ 28.,  27.,   7.],\n",
       "        ...,\n",
       "        [ 82.,  57.,  16.],\n",
       "        [ 31.,  29.,  17.],\n",
       "        [ 56.,  39.,  11.]],\n",
       "\n",
       "       [[  6.,   8.,   7.],\n",
       "        [ 67.,  57.,  45.],\n",
       "        [ 47.,  41.,  29.],\n",
       "        ...,\n",
       "        [107.,  61.,   9.],\n",
       "        [ 20.,  25.,  18.],\n",
       "        [112.,  91.,  34.]],\n",
       "\n",
       "       [[ 39.,  36.,  19.],\n",
       "        [ 19.,   9.,   0.],\n",
       "        [ 17.,  13.,   2.],\n",
       "        ...,\n",
       "        [126.,  80.,  20.],\n",
       "        [ 15.,  15.,  15.],\n",
       "        [ 74.,  65.,  26.]]], dtype=float32)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( f\"cat_test_image_arr.shape = { cat_test_image_arr.shape } and type( cat_test_image_arr ) = { type( cat_test_image_arr ) }\" )\n",
    "cat_test_image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog_test_image_arr.shape = (64, 64, 3) and type( dog_test_image_arr ) = <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 47.,  67.,  91.],\n",
       "        [ 33.,  57.,  83.],\n",
       "        [ 51.,  82., 102.],\n",
       "        ...,\n",
       "        [121., 130., 127.],\n",
       "        [113., 122., 117.],\n",
       "        [121., 130., 125.]],\n",
       "\n",
       "       [[ 11.,  38.,  65.],\n",
       "        [ 45.,  76., 107.],\n",
       "        [ 46.,  77., 108.],\n",
       "        ...,\n",
       "        [117., 126., 121.],\n",
       "        [117., 126., 121.],\n",
       "        [122., 131., 126.]],\n",
       "\n",
       "       [[ 17.,  37.,  61.],\n",
       "        [ 58.,  82., 108.],\n",
       "        [ 43.,  73.,  99.],\n",
       "        ...,\n",
       "        [114., 124., 115.],\n",
       "        [115., 124., 119.],\n",
       "        [121., 130., 125.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 33.,  51.,  75.],\n",
       "        [ 20.,  38.,  62.],\n",
       "        [ 22.,  38.,  63.],\n",
       "        ...,\n",
       "        [ 27.,  36.,   7.],\n",
       "        [ 42.,  57.,  26.],\n",
       "        [107., 119.,  97.]],\n",
       "\n",
       "       [[ 29.,  46.,  74.],\n",
       "        [ 22.,  39.,  67.],\n",
       "        [ 28.,  44.,  70.],\n",
       "        ...,\n",
       "        [ 15.,  23.,   8.],\n",
       "        [ 71.,  86.,  53.],\n",
       "        [ 48.,  61.,  31.]],\n",
       "\n",
       "       [[ 24.,  41.,  61.],\n",
       "        [ 22.,  38.,  63.],\n",
       "        [ 23.,  39.,  62.],\n",
       "        ...,\n",
       "        [ 18.,  21.,  12.],\n",
       "        [ 45.,  60.,  29.],\n",
       "        [ 46.,  61.,  30.]]], dtype=float32)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( f\"dog_test_image_arr.shape = { dog_test_image_arr.shape } and type( dog_test_image_arr ) = { type( dog_test_image_arr ) }\" )\n",
    "dog_test_image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (64, 64, 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-588b1347313e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Now, Let's Check the predicted Value for the above pixel image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mcat_test_image_arr\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#But, we will get Error, because, we need 4 dimension matrix not 3 dimesion.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1439\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1440\u001b[0m         \u001b[1;31m# Case 2: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1441\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1442\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1443\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 579\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\gauta\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    133\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    136\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (64, 64, 3)"
     ]
    }
   ],
   "source": [
    "#Now, Let's Check the predicted Value for the above pixel image.\n",
    "cnn.predict( cat_test_image_arr )\n",
    "\n",
    "#But, we will get Error, because, we need 4 dimension matrix not 3 dimesion.\n",
    "\n",
    "#Now the Question is why 4 dimension Matrix not 3 ?\n",
    "#Ans :- Because, CNN accepts image in a batch. Even if the no. of image is 1 and if it is in the batch then, it can predict.\n",
    "        #So, batch_size(1 dim) * image_dim( 3 dim ) = 4 dim_output\n",
    "    \n",
    "#Solution :- we need to expand our dimension using expand() available in numpy as our iamge pixel dataset is already in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[179., 184., 154.],\n",
       "        [185., 190., 160.],\n",
       "        [168., 173., 143.],\n",
       "        ...,\n",
       "        [ 97.,  80.,  26.],\n",
       "        [111.,  95.,  44.],\n",
       "        [119., 106.,  54.]],\n",
       "\n",
       "       [[185., 190., 160.],\n",
       "        [194., 199., 169.],\n",
       "        [172., 177., 147.],\n",
       "        ...,\n",
       "        [160., 139.,  72.],\n",
       "        [154., 136.,  72.],\n",
       "        [140., 126.,  61.]],\n",
       "\n",
       "       [[189., 194., 164.],\n",
       "        [198., 203., 173.],\n",
       "        [174., 179., 149.],\n",
       "        ...,\n",
       "        [167., 137.,  83.],\n",
       "        [158., 137.,  74.],\n",
       "        [147., 132.,  67.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[  1.,   3.,   2.],\n",
       "        [ 22.,  20.,   5.],\n",
       "        [ 28.,  27.,   7.],\n",
       "        ...,\n",
       "        [ 82.,  57.,  16.],\n",
       "        [ 31.,  29.,  17.],\n",
       "        [ 56.,  39.,  11.]],\n",
       "\n",
       "       [[  6.,   8.,   7.],\n",
       "        [ 67.,  57.,  45.],\n",
       "        [ 47.,  41.,  29.],\n",
       "        ...,\n",
       "        [107.,  61.,   9.],\n",
       "        [ 20.,  25.,  18.],\n",
       "        [112.,  91.,  34.]],\n",
       "\n",
       "       [[ 39.,  36.,  19.],\n",
       "        [ 19.,   9.,   0.],\n",
       "        [ 17.,  13.,   2.],\n",
       "        ...,\n",
       "        [126.,  80.,  20.],\n",
       "        [ 15.,  15.,  15.],\n",
       "        [ 74.,  65.,  26.]]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_test_image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_test_image_arr = np.expand_dims( cat_test_image_arr, axis = 0 )\n",
    "dog_test_image_arr = np.expand_dims( dog_test_image_arr, axis = 0 )\n",
    "\n",
    "#Why axis = 0\n",
    "#Ans :- axis is 0 because, we have only 1 mage to test for now, so, we are doing axis = 0 means axis = row. It means,\n",
    "#    We are selecting row wise, and we have only one row.\n",
    "#    second image will have second row, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat_test_image_arr.shape = (1, 64, 64, 3)\n",
      "dog_test_image_arr.shape = (1, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[179., 184., 154.],\n",
       "         [185., 190., 160.],\n",
       "         [168., 173., 143.],\n",
       "         ...,\n",
       "         [ 97.,  80.,  26.],\n",
       "         [111.,  95.,  44.],\n",
       "         [119., 106.,  54.]],\n",
       "\n",
       "        [[185., 190., 160.],\n",
       "         [194., 199., 169.],\n",
       "         [172., 177., 147.],\n",
       "         ...,\n",
       "         [160., 139.,  72.],\n",
       "         [154., 136.,  72.],\n",
       "         [140., 126.,  61.]],\n",
       "\n",
       "        [[189., 194., 164.],\n",
       "         [198., 203., 173.],\n",
       "         [174., 179., 149.],\n",
       "         ...,\n",
       "         [167., 137.,  83.],\n",
       "         [158., 137.,  74.],\n",
       "         [147., 132.,  67.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[  1.,   3.,   2.],\n",
       "         [ 22.,  20.,   5.],\n",
       "         [ 28.,  27.,   7.],\n",
       "         ...,\n",
       "         [ 82.,  57.,  16.],\n",
       "         [ 31.,  29.,  17.],\n",
       "         [ 56.,  39.,  11.]],\n",
       "\n",
       "        [[  6.,   8.,   7.],\n",
       "         [ 67.,  57.,  45.],\n",
       "         [ 47.,  41.,  29.],\n",
       "         ...,\n",
       "         [107.,  61.,   9.],\n",
       "         [ 20.,  25.,  18.],\n",
       "         [112.,  91.,  34.]],\n",
       "\n",
       "        [[ 39.,  36.,  19.],\n",
       "         [ 19.,   9.,   0.],\n",
       "         [ 17.,  13.,   2.],\n",
       "         ...,\n",
       "         [126.,  80.,  20.],\n",
       "         [ 15.,  15.,  15.],\n",
       "         [ 74.,  65.,  26.]]]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( f\"cat_test_image_arr.shape = { cat_test_image_arr.shape }\" )\n",
    "print( f\"dog_test_image_arr.shape = { dog_test_image_arr.shape }\" )\n",
    "cat_test_image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 47.,  67.,  91.],\n",
       "         [ 33.,  57.,  83.],\n",
       "         [ 51.,  82., 102.],\n",
       "         ...,\n",
       "         [121., 130., 127.],\n",
       "         [113., 122., 117.],\n",
       "         [121., 130., 125.]],\n",
       "\n",
       "        [[ 11.,  38.,  65.],\n",
       "         [ 45.,  76., 107.],\n",
       "         [ 46.,  77., 108.],\n",
       "         ...,\n",
       "         [117., 126., 121.],\n",
       "         [117., 126., 121.],\n",
       "         [122., 131., 126.]],\n",
       "\n",
       "        [[ 17.,  37.,  61.],\n",
       "         [ 58.,  82., 108.],\n",
       "         [ 43.,  73.,  99.],\n",
       "         ...,\n",
       "         [114., 124., 115.],\n",
       "         [115., 124., 119.],\n",
       "         [121., 130., 125.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 33.,  51.,  75.],\n",
       "         [ 20.,  38.,  62.],\n",
       "         [ 22.,  38.,  63.],\n",
       "         ...,\n",
       "         [ 27.,  36.,   7.],\n",
       "         [ 42.,  57.,  26.],\n",
       "         [107., 119.,  97.]],\n",
       "\n",
       "        [[ 29.,  46.,  74.],\n",
       "         [ 22.,  39.,  67.],\n",
       "         [ 28.,  44.,  70.],\n",
       "         ...,\n",
       "         [ 15.,  23.,   8.],\n",
       "         [ 71.,  86.,  53.],\n",
       "         [ 48.,  61.,  31.]],\n",
       "\n",
       "        [[ 24.,  41.,  61.],\n",
       "         [ 22.,  38.,  63.],\n",
       "         [ 23.,  39.,  62.],\n",
       "         ...,\n",
       "         [ 18.,  21.,  12.],\n",
       "         [ 45.,  60.,  29.],\n",
       "         [ 46.,  61.,  30.]]]], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog_test_image_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int( cnn.predict( cat_test_image_arr ) ) = 1\n",
      "int( cnn.predict( dog_test_image_arr ) ) = 0\n"
     ]
    }
   ],
   "source": [
    "print( f\"int( cnn.predict( cat_test_image_arr ) ) = { int( cnn.predict( cat_test_image_arr ) ) }\" )\n",
    "print( f\"int( cnn.predict( dog_test_image_arr ) ) = { int( cnn.predict( dog_test_image_arr ) ) }\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now the Question is which image is for Cat and which one is for dog ?\n",
    "\n",
    "#Ans :- we just have to use class_indices. Syntax :- train_var.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Cat': 0, 'Dog': 1}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
